{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List,Dict\n",
    "\n",
    "import copy\n",
    "import itertools \n",
    "import time\n",
    "from tampura.policies.policy import save_config, RolloutHistory, save_run_data\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from tampura.environment import TampuraEnv\n",
    "from tampura.spec import ProblemSpec\n",
    "from tampura.structs import (\n",
    "    AbstractBelief,\n",
    "    ActionSchema,\n",
    "    StreamSchema,\n",
    "    AliasStore,\n",
    "    Belief,\n",
    "    NoOp,\n",
    "    Predicate,\n",
    "    State,\n",
    "    effect_from_execute_fn,\n",
    "    Observation,\n",
    "    AbstractBeliefSet,\n",
    ")\n",
    "import logging \n",
    "from tampura.symbolic import OBJ, Atom, ForAll, Not, Exists, Or, And, OneOf, eval_expr\n",
    "from tampura.policies.tampura_policy import TampuraPolicy\n",
    "from tampura.config.config import load_config, setup_logger\n",
    "\n",
    "ROB = \"robot_\"\n",
    "REG = \"region_\"\n",
    "MUG = \"mug\"\n",
    "DOOR = \"door\"\n",
    "REGIONS = [f\"{REG}{MUG}\",f\"{REG}{DOOR}\",f\"{REG}stable_mug\"]\n",
    "ACTION_NAMES = [\"transit_action\",\"transfer_action\",\"pick_action\",\"place_action\",\"open_action\",\"close_action\",\"nothing_action\"]\n",
    "\n",
    "# problem specification: try with just one robot to demonstrate how overall cost increases\n",
    "ROBOTS=[f\"{ROB}1\",f\"{ROB}2\"]\n",
    "ROB_REGIONS = {ROBOTS[0]:REGIONS[-1],ROBOTS[1]:REGIONS[-1]} # long horizon: combinatorial explosion\n",
    "# ROB_REGIONS = {ROBOTS[0]:REGIONS[1],ROBOTS[1]:REGIONS[0]} # short horizon: kind of works?\n",
    "OBJ_REGIONS={MUG:REGIONS[0]}\n",
    "\n",
    "# Test \n",
    "GOAL = And([Exists(Atom(\"holding\",[\"?rob\",MUG]),[\"?rob\"],[\"robot\"]),Not(Atom(\"open\",[DOOR]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralized planner\n",
    "# State of the environment\n",
    "\n",
    "# Belief space\n",
    "class CentralBelief(Belief):\n",
    "    def __init__(self, holding={},open_door=False,rob_regions={},obj_regions={},turn=ROBOTS[0]):\n",
    "        # true state\n",
    "        self.holding = holding\n",
    "        self.open_door = open_door\n",
    "        self.rob_regions = rob_regions\n",
    "        self.obj_regions = obj_regions\n",
    "        self.turn = turn\n",
    "        \n",
    "\n",
    "    def update(self, a, o, s):\n",
    "        \n",
    "        # dictionary mutations are IN-PLACE!!! use .copy()!!\n",
    "        holding = self.holding.copy() \n",
    "        open_door = self.open_door\n",
    "        rob_regions = self.rob_regions.copy()\n",
    "        obj_regions = self.obj_regions.copy()\n",
    "        turn = self.turn\n",
    "        \n",
    "        \n",
    "        # BE CAREFUL: update names if you change action schema names\n",
    "        if a.name == \"pick\":\n",
    "            holding[a.args[0]]=[a.args[1]]\n",
    "            obj_regions[a.args[1]]=\"\"\n",
    "        elif a.name == \"place\":\n",
    "            holding[a.args[0]]=[]\n",
    "            obj_regions[a.args[1]]=a.args[2]\n",
    "        elif a.name == \"transit\" or a.name == \"transfer\":\n",
    "            rob_regions[a.args[0]]=a.args[2]\n",
    "        elif a.name == \"open\":\n",
    "            open_door=True\n",
    "        elif a.name == \"close\":\n",
    "            open_door=False\n",
    "        \n",
    "        turn=a.args[-1] # turn of the agent\n",
    "            \n",
    "        return CentralBelief(holding=holding,open_door=open_door,rob_regions=rob_regions,obj_regions=obj_regions,turn=turn)\n",
    "\n",
    "    def abstract(self, store: AliasStore):\n",
    "        \n",
    "        ab = []\n",
    "        \n",
    "        # true state\n",
    "        for rob in self.holding.keys():\n",
    "            ab += [Atom(\"holding\",[rob,obj]) for obj in self.holding[rob]]\n",
    "        for rob in self.rob_regions.keys():\n",
    "            ab += [Atom(\"in_rob\",[rob,self.rob_regions[rob]])]\n",
    "        for obj in self.obj_regions.keys():\n",
    "            if self.obj_regions[obj] !=\"\":\n",
    "                ab += [Atom(\"in_obj\",[obj,self.obj_regions[obj]])]\n",
    "        if self.open_door:\n",
    "            ab += [Atom(\"open\",[DOOR])]\n",
    "        \n",
    "        ab += [Atom(\"turn\",[self.turn])]\n",
    "            \n",
    "        return AbstractBelief(ab)\n",
    "\n",
    "    # def vectorize(self):\n",
    "    #     return np.array([int(obj in self.holding) for obj in OBJECTS])\n",
    "      \n",
    "\n",
    "def deterministic_execute_fn(a, b, s, store):\n",
    "    return State(), Observation()\n",
    "    \n",
    "def deterministic_effects_fn(a, b, store):\n",
    "    o = Observation()    \n",
    "    new_belief=b.update(a,o,store)\n",
    "    return AbstractBeliefSet.from_beliefs([new_belief], store)\n",
    "\n",
    "# Set up environment dynamics\n",
    "class ToyDiscreteCentral(TampuraEnv):\n",
    "    \n",
    "    def initialize(self,holding,open_door,rob_regions,obj_regions,turn):\n",
    "        \n",
    "        store = AliasStore()\n",
    "        \n",
    "        for rob in ROBOTS:\n",
    "            \n",
    "            store.set(rob, rob, \"robot\")\n",
    "        # store.set(ego,ego,\"robot\")\n",
    "            \n",
    "        for region in REGIONS:\n",
    "            store.set(region, region, \"region\")\n",
    "        \n",
    "        store.set(MUG, MUG, \"physical\")\n",
    "        store.set(DOOR, DOOR, \"door\")\n",
    "        \n",
    "        store.certified.append(Atom(\"stable\",[MUG,REGIONS[0]]))\n",
    "        store.certified.append(Atom(\"stable\",[MUG,REGIONS[2]]))\n",
    "\n",
    "        b = CentralBelief(holding=holding,open_door=open_door,rob_regions=rob_regions,obj_regions=obj_regions,turn=turn)\n",
    "\n",
    "        return b, store\n",
    "\n",
    "    def get_problem_spec(self) -> ProblemSpec:\n",
    "        \n",
    "\n",
    "        predicates = [\n",
    "            \n",
    "            Predicate(\"holding\", [\"robot\",\"physical\"]),\n",
    "            Predicate(\"stable\",[\"physical\",\"region\"]),\n",
    "            Predicate(\"in_rob\",[\"robot\",\"region\"]),\n",
    "            Predicate(\"in_obj\",[\"physical\",\"region\"]),\n",
    "            Predicate(\"open\",[\"door\"]),\n",
    "            Predicate(\"turn\",[\"robot\"])\n",
    "        ] \n",
    "        \n",
    "        # modify preconditions, effects and execute functions for observation\n",
    "        action_schemas = [\n",
    "            \n",
    "            # ego-agent\n",
    "            ActionSchema(\n",
    "                name=\"pick\",\n",
    "                inputs=[\"?rob1\",\"?obj1\",\"?reg1\",\"?rob2\"],\n",
    "                input_types=[\"robot\",\"physical\",\"region\",\"robot\"],\n",
    "                preconditions=[Atom(\"turn\",[\"?rob1\"]),Not(Atom(\"turn\",[\"?rob2\"])),\n",
    "                               Or([Not(Atom(\"in_obj\",[\"?obj1\",REGIONS[0]])),And([Atom(\"in_obj\",[\"?obj1\",REGIONS[0]]),Atom(\"open\",[DOOR])])]), # TODO: modify!! accesibility of mug: derived predicate\n",
    "                               Atom(\"in_obj\",[\"?obj1\",\"?reg1\"]), # object is in region from where pick is attempted\n",
    "                               Atom(\"in_rob\",[\"?rob1\",\"?reg1\"]), # robot is in region from where pick is attempted\n",
    "                               Not(Exists(Atom(\"holding\",[\"?rob1\",\"?obj\"]),[\"?obj\"],[\"physical\"])), # robot hand is free\n",
    "                               ],\n",
    "                effects=[Atom(\"holding\",[\"?rob1\",\"?obj1\"]),Not(Atom(\"in_obj\",[\"?obj1\",\"?reg1\"])),\n",
    "                         Atom(\"turn\",[\"?rob2\"]),Not(Atom(\"turn\",[\"?rob1\"]))], # deterministic\n",
    "                effects_fn=deterministic_effects_fn,\n",
    "                execute_fn=deterministic_execute_fn,\n",
    "            ),\n",
    "            \n",
    "            \n",
    "            ActionSchema(\n",
    "                name=\"place\",\n",
    "                inputs=[\"?rob1\",\"?obj1\",\"?reg1\",\"?rob2\"],\n",
    "                input_types=[\"robot\",\"physical\",\"region\",\"robot\"],\n",
    "                preconditions=[Atom(\"turn\",[\"?rob1\"]),Not(Atom(\"turn\",[\"?rob2\"])),\n",
    "                               Or([Not(Atom(\"in_rob\",[\"?rob1\",REGIONS[0]])),And([Atom(\"in_rob\",[\"?rob1\",REGIONS[0]]),Atom(\"open\",[DOOR])])]), # TODO: modify!! accessibility of region\n",
    "                               Atom(\"in_rob\",[\"?rob1\",\"?reg1\"]), # robot is in region where place is attempted\n",
    "                               Atom(\"holding\",[\"?rob1\",\"?obj1\"]), # robot is holding the object that is to be placed \n",
    "                               Atom(\"stable\",[\"?obj1\",\"?reg1\"]), # region where place is attempted is stable\n",
    "                               ],\n",
    "                effects=[Not(Atom(\"holding\",[\"?rob1\",\"?obj1\"])),Atom(\"in_obj\",[\"?obj1\",\"?reg1\"]),\n",
    "                         Atom(\"turn\",[\"?rob2\"]),Not(Atom(\"turn\",[\"?rob1\"]))], # deterministic \n",
    "                effects_fn=deterministic_effects_fn,\n",
    "                execute_fn=deterministic_execute_fn,\n",
    "            ),\n",
    "            \n",
    "\n",
    "            ActionSchema(\n",
    "                name=\"transit\",\n",
    "                inputs=[\"?rob1\",\"?reg1\",\"?reg2\",\"?rob2\"],\n",
    "                input_types=[\"robot\",\"region\",\"region\",\"robot\"],\n",
    "                preconditions=[Atom(\"turn\",[\"?rob1\"]),Not(Atom(\"turn\",[\"?rob2\"])),\n",
    "                               Atom(\"in_rob\",[\"?rob1\",\"?reg1\"]),\n",
    "                               Not(Atom(\"in_rob\",[\"?rob1\",\"?reg2\"])),\n",
    "                               Not(Exists(Atom(\"holding\",[\"?rob1\",\"?obj\"]),[\"?obj\"],[\"physical\"])), # robot hand is free\n",
    "                               ],\n",
    "                effects=[Not(Atom(\"in_rob\",[\"?rob1\",\"?reg1\"])),Atom(\"in_rob\",[\"?rob1\",\"?reg2\"]),\n",
    "                         Atom(\"turn\",[\"?rob2\"]),Not(Atom(\"turn\",[\"?rob1\"]))],\n",
    "                effects_fn=deterministic_effects_fn,\n",
    "                execute_fn=deterministic_execute_fn,\n",
    "            ),\n",
    "            ActionSchema(\n",
    "                name=\"transfer\",\n",
    "                inputs=[\"?rob1\",\"?reg1\",\"?reg2\",\"?obj1\",\"?rob2\"],\n",
    "                input_types=[\"robot\",\"region\",\"region\",\"physical\",\"robot\"],\n",
    "                preconditions=[Atom(\"turn\",[\"?rob1\"]),Not(Atom(\"turn\",[\"?rob2\"])),\n",
    "                               Atom(\"in_rob\",[\"?rob1\",\"?reg1\"]),\n",
    "                               Not(Atom(\"in_rob\",[\"?rob1\",\"?reg2\"])),\n",
    "                               Atom(\"holding\",[\"?rob1\",\"?obj1\"])],\n",
    "                effects=[Not(Atom(\"in_rob\",[\"?rob1\",\"?reg1\"])),Atom(\"in_rob\",[\"?rob1\",\"?reg2\"]),\n",
    "                         Atom(\"turn\",[\"?rob2\"]),Not(Atom(\"turn\",[\"?rob1\"]))],\n",
    "                effects_fn=deterministic_effects_fn,\n",
    "                execute_fn=deterministic_execute_fn,\n",
    "            ),\n",
    "            ActionSchema(\n",
    "                name=\"open\",\n",
    "                inputs=[\"?rob1\",\"?rob2\"],\n",
    "                input_types=[\"robot\",\"robot\"],\n",
    "                preconditions=[Atom(\"turn\",[\"?rob1\"]),Not(Atom(\"turn\",[\"?rob2\"])),\n",
    "                               Not(Atom(\"open\",[DOOR])),\n",
    "                               Atom(\"in_rob\",[\"?rob1\",REGIONS[1]]),\n",
    "                               Not(Exists(Atom(\"holding\",[\"?rob1\",\"?obj\"]),[\"?obj\"],[\"physical\"]))],\n",
    "                effects=[Atom(\"open\",[DOOR]),\n",
    "                         Atom(\"turn\",[\"?rob2\"]),Not(Atom(\"turn\",[\"?rob1\"]))],\n",
    "                effects_fn=deterministic_effects_fn,\n",
    "                execute_fn=deterministic_execute_fn,\n",
    "            ),\n",
    "            ActionSchema(\n",
    "                name=\"close\",\n",
    "                inputs=[\"?rob1\",\"?rob2\"],\n",
    "                input_types=[\"robot\",\"robot\"],\n",
    "                preconditions=[Atom(\"turn\",[\"?rob1\"]),Not(Atom(\"turn\",[\"?rob2\"])), \n",
    "                               Atom(\"open\",[DOOR]),\n",
    "                               Atom(\"in_rob\",[\"?rob1\",REGIONS[1]]),\n",
    "                               Not(Exists(Atom(\"holding\",[\"?rob1\",\"?obj\"]),[\"?obj\"],[\"physical\"]))],\n",
    "                effects=[Not(Atom(\"open\",[DOOR])),\n",
    "                         Atom(\"turn\",[\"?rob2\"]),Not(Atom(\"turn\",[\"?rob1\"]))],\n",
    "                effects_fn=deterministic_effects_fn,\n",
    "                execute_fn=deterministic_execute_fn,\n",
    "\n",
    "            ),\n",
    "            \n",
    "            ActionSchema(\n",
    "                name=\"nothing\",\n",
    "                inputs=[\"?rob1\",\"?rob2\"],\n",
    "                input_types=[\"robot\",\"robot\"],\n",
    "                preconditions=[Atom(\"turn\",[\"?rob1\"]),Not(Atom(\"turn\",[\"?rob2\"]))],\n",
    "                effects=[Atom(\"turn\",[\"?rob2\"]),Not(Atom(\"turn\",[\"?rob1\"]))],\n",
    "                effects_fn=deterministic_effects_fn,\n",
    "                execute_fn=deterministic_execute_fn,\n",
    "            ),\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        reward = GOAL\n",
    "\n",
    "        spec = ProblemSpec(\n",
    "            predicates=predicates,\n",
    "            action_schemas=action_schemas,\n",
    "            reward=reward,\n",
    "        )\n",
    "\n",
    "        return spec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "# Planner\n",
    "cfg = load_config(config_file=\"../tampura/config/default.yml\")\n",
    "\n",
    "# Set some print options to print out abstract belief, action, observation, and reward\n",
    "cfg[\"print_options\"] = \"ab,a,o,r\"\n",
    "cfg[\"vis_graph\"] = True\n",
    "# batch size 100, num samples 500 num skeletons 100 works best!!\n",
    "cfg[\"batch_size\"] = 100 #100 \n",
    "cfg[\"num_samples\"] = 100#500\n",
    "cfg[\"max_steps\"] = 15\n",
    "cfg[\"num_skeletons\"] = 10\n",
    "cfg[\"flat_sample\"] = False # TODO: check; may cause progressive widening\n",
    "cfg['save_dir'] = os.getcwd()+\"/runs/run{}\".format(time.time())\n",
    "\n",
    "# cfg['from_scratch'] = False # imp: re-use!!! but graph gets too big\n",
    "\n",
    "# TODO: check - can we reuse the same environment for both agents?\n",
    "# for robot1\n",
    "# Initialize environment\n",
    "env = ToyDiscreteCentral(config=cfg)\n",
    "b0, store= env.initialize(holding={ROBOTS[0]:[],ROBOTS[1]:[]},open_door=False,\n",
    "                          rob_regions={ROBOTS[0]:REGIONS[-1],ROBOTS[1]:REGIONS[-1]},\n",
    "                          obj_regions={MUG:REGIONS[0]},turn=ROBOTS[0])\n",
    "\n",
    "# Set up logger to print info\n",
    "setup_logger(cfg[\"save_dir\"], logging.INFO)\n",
    "\n",
    "# Initialize the policy\n",
    "planner = TampuraPolicy(config = cfg, problem_spec = env.problem_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========t=0==========\n",
      "Abstract Belief: AbstractBelief(items=[Atom(pred_name='in_rob', args=['robot_2', 'region_stable_mug']), Atom(pred_name='in_rob', args=['robot_1', 'region_stable_mug']), Atom(pred_name='in_obj', args=['mug', 'region_mug']), Atom(pred_name='turn', args=['robot_1'])])\n",
      "Reward: 0.0\n",
      "[TampuraPolicy] Flat Action Parameter Sampling\n",
      "[TampuraPolicy] Outcome Sampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 156.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TampuraPolicy] MDP Solving\n",
      "Action: transit(robot_1, region_stable_mug, region_door, robot_2)\n",
      "Observation: Observation()\n",
      "\n",
      "==========t=1==========\n",
      "Abstract Belief: AbstractBelief(items=[Atom(pred_name='in_rob', args=['robot_2', 'region_stable_mug']), Atom(pred_name='in_obj', args=['mug', 'region_mug']), Atom(pred_name='in_rob', args=['robot_1', 'region_door']), Atom(pred_name='turn', args=['robot_2'])])\n",
      "Reward: 0.0\n",
      "[TampuraPolicy] MDP Solving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: transit(robot_2, region_stable_mug, region_mug, robot_1)\n",
      "Observation: Observation()\n",
      "\n",
      "==========t=2==========\n",
      "Abstract Belief: AbstractBelief(items=[Atom(pred_name='in_rob', args=['robot_2', 'region_mug']), Atom(pred_name='in_obj', args=['mug', 'region_mug']), Atom(pred_name='turn', args=['robot_1']), Atom(pred_name='in_rob', args=['robot_1', 'region_door'])])\n",
      "Reward: 0.0\n",
      "[TampuraPolicy] MDP Solving\n",
      "Action: open(robot_1, robot_2)\n",
      "Observation: Observation()\n",
      "\n",
      "==========t=3==========\n",
      "Abstract Belief: AbstractBelief(items=[Atom(pred_name='in_rob', args=['robot_2', 'region_mug']), Atom(pred_name='open', args=['door']), Atom(pred_name='in_obj', args=['mug', 'region_mug']), Atom(pred_name='in_rob', args=['robot_1', 'region_door']), Atom(pred_name='turn', args=['robot_2'])])\n",
      "Reward: 0.0\n",
      "[TampuraPolicy] MDP Solving\n",
      "Action: pick(robot_2, mug, region_mug, robot_1)\n",
      "Observation: Observation()\n",
      "\n",
      "==========t=4==========\n",
      "Abstract Belief: AbstractBelief(items=[Atom(pred_name='in_rob', args=['robot_2', 'region_mug']), Atom(pred_name='open', args=['door']), Atom(pred_name='turn', args=['robot_1']), Atom(pred_name='holding', args=['robot_2', 'mug']), Atom(pred_name='in_rob', args=['robot_1', 'region_door'])])\n",
      "Reward: 0.0\n",
      "[TampuraPolicy] MDP Solving\n",
      "Action: close(robot_1, robot_2)\n",
      "Observation: Observation()\n",
      "goal achieved\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "b=b0\n",
    "assert env.problem_spec.verify(store)\n",
    "\n",
    "save_config(planner.config, planner.config[\"save_dir\"])\n",
    "\n",
    "history = RolloutHistory(planner.config)\n",
    "st = time.time()\n",
    "for step in range(100):\n",
    "# while True:\n",
    "    s = copy.deepcopy(env.state)\n",
    "    a_b = b.abstract(store)\n",
    "    reward = env.problem_spec.get_reward(a_b, store)\n",
    "    if reward:\n",
    "        print(\"goal achieved\")\n",
    "        break\n",
    "\n",
    "    logging.info(\"\\n\" + (\"=\" * 10) + \"t=\" + str(step) + (\"=\" * 10))\n",
    "    if \"s\" in planner.print_options:\n",
    "        logging.info(\"State: \" + str(s))\n",
    "    if \"b\" in planner.print_options:\n",
    "        logging.info(\"Belief: \" + str(b))\n",
    "    if \"ab\" in planner.print_options:\n",
    "        logging.info(\"Abstract Belief: \" + str(a_b))\n",
    "    if \"r\" in planner.print_options:\n",
    "        logging.info(\"Reward: \" + str(reward))\n",
    "    \n",
    "    \n",
    "    action, info, store = planner.get_action(b, store) # should only call effects functions!!??\n",
    "    \n",
    "    \n",
    "    if \"a\" in planner.print_options:\n",
    "        logging.info(\"Action: \" + str(action))\n",
    "\n",
    "    if action.name == \"no-op\":\n",
    "        bp = copy.deepcopy(b)\n",
    "        observation = None\n",
    "    else:\n",
    "        observation = env.step(action, b, store) # should call execute function\n",
    "        bp = b.update(action, observation, store)\n",
    "\n",
    "        if planner.config[\"vis\"]:\n",
    "            env.vis_updated_belief(bp, store)\n",
    "\n",
    "    a_bp = bp.abstract(store)\n",
    "    history.add(s, b, a_b, action, observation, reward, info, store, time.time() - st)\n",
    "\n",
    "    reward = env.problem_spec.get_reward(a_bp, store)\n",
    "    \n",
    "    if \"o\" in planner.print_options:\n",
    "        logging.info(\"Observation: \" + str(observation))\n",
    "    if \"sp\" in planner.print_options:\n",
    "        logging.info(\"Next State: \" + str(env.state))\n",
    "    if \"bp\" in planner.print_options:\n",
    "        logging.info(\"Next Belief: \" + str(bp))\n",
    "    if \"abp\" in planner.print_options:\n",
    "        logging.info(\"Next Abstract Belief: \" + str(a_bp))\n",
    "    if \"rp\" in planner.print_options:\n",
    "        logging.info(\"Next Reward: \" + str(reward))\n",
    "\n",
    "    # update the belief\n",
    "    b = bp\n",
    "\n",
    "history.add(env.state, bp, a_bp, None, None, reward, info, store, time.time() - st)\n",
    "\n",
    "logging.info(\"=\" * 20)\n",
    "\n",
    "env.wrapup()\n",
    "\n",
    "if not planner.config[\"real_execute\"]:\n",
    "    save_run_data(history, planner.config[\"save_dir\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/am/tamp_multi_agent/notebooks/runs/run1748414549.6519341'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg['save_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_isaaclab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
