{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List,Dict\n",
    "\n",
    "import copy\n",
    "import itertools \n",
    "import time\n",
    "from tampura.policies.policy import save_config, RolloutHistory, save_run_data\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from tampura.environment import TampuraEnv\n",
    "from tampura.spec import ProblemSpec\n",
    "from tampura.structs import (\n",
    "    AbstractBelief,\n",
    "    ActionSchema,\n",
    "    StreamSchema,\n",
    "    AliasStore,\n",
    "    Belief,\n",
    "    NoOp,\n",
    "    Predicate,\n",
    "    State,\n",
    "    effect_from_execute_fn,\n",
    "    Observation,\n",
    "    AbstractBeliefSet,\n",
    ")\n",
    "import logging \n",
    "from tampura.symbolic import OBJ, Atom, ForAll, Not, Exists, Or, And, OneOf, eval_expr\n",
    "from tampura.policies.tampura_policy import TampuraPolicy\n",
    "from tampura.config.config import load_config, setup_logger\n",
    "\n",
    "ROB = \"robot_\"\n",
    "REG = \"region_\"\n",
    "MUG = \"mug\"\n",
    "DOOR = \"door\"\n",
    "REGIONS = [f\"{REG}{MUG}\",f\"{REG}{DOOR}\",f\"{REG}stable_mug\"]\n",
    "ACTION_NAMES = [\"transit_action\",\"transfer_action\",\"pick_action\",\"place_action\",\"open_action\",\"close_action\",\"nothing_action\"]\n",
    "\n",
    "# problem specification: try with just one robot to demonstrate how overall cost increases\n",
    "ROBOTS=[f\"{ROB}1\",f\"{ROB}2\"]\n",
    "ROB_REGIONS = {ROBOTS[0]:REGIONS[-1],ROBOTS[1]:REGIONS[-1]} # long horizon: combinatorial explosion\n",
    "# ROB_REGIONS = {ROBOTS[0]:REGIONS[1],ROBOTS[1]:REGIONS[0]} # short horizon: kind of works?\n",
    "OBJ_REGIONS={MUG:REGIONS[0]}\n",
    "\n",
    "# Test \n",
    "GOAL = And([Atom(\"clean\",[REGIONS[0]]),Atom(\"in_obj\",[MUG,REGIONS[0]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralized planner\n",
    "# State of the environment\n",
    "\n",
    "# Belief space\n",
    "class CentralBelief(Belief):\n",
    "    \n",
    "    def __init__(self, holding={},obj_regions={},clean=[],next_actions=[],turn=ROBOTS[0]):\n",
    "        # true state\n",
    "        self.holding = holding.copy()\n",
    "        self.obj_regions = obj_regions.copy()\n",
    "        self.clean = clean.copy()\n",
    "        self.turn = turn\n",
    "    \n",
    "        \n",
    "\n",
    "    def update(self, a, o, s):\n",
    "        \n",
    "        # dictionary mutations are IN-PLACE!!! use .copy()!!\n",
    "        holding = self.holding.copy() \n",
    "        obj_regions = self.obj_regions.copy()\n",
    "        clean = self.clean.copy()\n",
    "        turn = self.turn\n",
    "        \n",
    "        \n",
    "        # BE CAREFUL: update names if you change action schema names\n",
    "        if a.name == \"pick\":\n",
    "            holding[a.args[0]]=[a.args[1]]\n",
    "            obj_regions[a.args[1]]=\"\"\n",
    "        elif a.name == \"place\":\n",
    "            holding[a.args[0]]=[]\n",
    "            obj_regions[a.args[1]]=a.args[2]\n",
    "        elif a.name == \"clean\":\n",
    "            clean.append(a.args[1])\n",
    "            \n",
    "        turn=a.args[-1] # turn of the agent\n",
    "            \n",
    "        return CentralBelief(holding=holding,clean=clean,obj_regions=obj_regions,turn=turn)\n",
    "\n",
    "    def abstract(self, store: AliasStore):\n",
    "        \n",
    "        ab = []\n",
    "        \n",
    "        # true state\n",
    "        for rob in self.holding.keys():\n",
    "            ab += [Atom(\"holding\",[rob,obj]) for obj in self.holding[rob]]\n",
    "        \n",
    "        for obj in self.obj_regions.keys():\n",
    "            if self.obj_regions[obj] !=\"\":\n",
    "                ab += [Atom(\"in_obj\",[obj,self.obj_regions[obj]])]\n",
    "        \n",
    "        for reg in self.clean:\n",
    "            ab += [Atom(\"clean\",[reg])]\n",
    "        \n",
    "        ab += [Atom(\"turn\",[self.turn])]\n",
    "            \n",
    "        return AbstractBelief(ab)\n",
    "\n",
    "    # def vectorize(self):\n",
    "    #     return np.array([int(obj in self.holding) for obj in OBJECTS])\n",
    "      \n",
    "\n",
    "def deterministic_execute_fn(a, b, s, store):\n",
    "    return State(), Observation()\n",
    "    \n",
    "def deterministic_effects_fn(a, b, store):\n",
    "    o = Observation()    \n",
    "    new_belief=b.update(a,o,store)\n",
    "    return AbstractBeliefSet.from_beliefs([new_belief], store)\n",
    "\n",
    "# Set up environment dynamics\n",
    "class ToyDiscreteCentral(TampuraEnv):\n",
    "    \n",
    "    def initialize(self,holding,obj_regions,clean,turn):\n",
    "        \n",
    "        store = AliasStore()\n",
    "        \n",
    "        for rob in ROBOTS:\n",
    "            \n",
    "            store.set(rob, rob, \"robot\")\n",
    "        # store.set(ego,ego,\"robot\")\n",
    "            \n",
    "        for region in REGIONS:\n",
    "            store.set(region, region, \"region\")\n",
    "        \n",
    "        store.set(MUG, MUG, \"physical\")\n",
    "        \n",
    "        store.certified.append(Atom(\"stable\",[MUG,REGIONS[0]]))\n",
    "        store.certified.append(Atom(\"stable\",[MUG,REGIONS[2]]))\n",
    "        \n",
    "\n",
    "        b = CentralBelief(holding=holding,obj_regions=obj_regions,clean=clean,turn=turn)\n",
    "\n",
    "        return b, store\n",
    "\n",
    "    def get_problem_spec(self) -> ProblemSpec:\n",
    "        \n",
    "\n",
    "        predicates = [\n",
    "            \n",
    "            Predicate(\"holding\", [\"robot\",\"physical\"]),\n",
    "            Predicate(\"stable\",[\"physical\",\"region\"]),\n",
    "            Predicate(\"in_obj\",[\"physical\",\"region\"]),\n",
    "            Predicate(\"clean\",[\"region\"]),\n",
    "            Predicate(\"turn\",[\"robot\"])\n",
    "        ] \n",
    "        \n",
    "        # modify preconditions, effects and execute functions for observation\n",
    "        action_schemas = [\n",
    "            \n",
    "            # ego-agent\n",
    "            ActionSchema(\n",
    "                name=\"pick\",\n",
    "                inputs=[\"?rob1\",\"?obj1\",\"?reg1\",\"?rob2\"],\n",
    "                input_types=[\"robot\",\"physical\",\"region\",\"robot\"],\n",
    "                preconditions=[Atom(\"turn\",[\"?rob1\"]),Not(Atom(\"turn\",[\"?rob2\"])),\n",
    "                               Atom(\"in_obj\",[\"?obj1\",\"?reg1\"]), # object is in region from where pick is attempted\n",
    "                               Not(Exists(Atom(\"holding\",[\"?rob1\",\"?obj\"]),[\"?obj\"],[\"physical\"])), # robot hand is free\n",
    "                               ],\n",
    "                effects=[Atom(\"holding\",[\"?rob1\",\"?obj1\"]),Not(Atom(\"in_obj\",[\"?obj1\",\"?reg1\"])),Atom(\"turn\",[\"?rob2\"]),Not(Atom(\"turn\",[\"?rob1\"]))],\n",
    "                execute_fn=deterministic_execute_fn,\n",
    "                effects_fn=deterministic_effects_fn,\n",
    "            ),\n",
    "            \n",
    "            \n",
    "            ActionSchema(\n",
    "                name=\"place\",\n",
    "                inputs=[\"?rob1\",\"?obj1\",\"?reg1\",\"?rob2\"],\n",
    "                input_types=[\"robot\",\"physical\",\"region\",\"robot\"],\n",
    "                preconditions=[Atom(\"turn\",[\"?rob1\"]),Not(Atom(\"turn\",[\"?rob2\"])),\n",
    "                               Not(Atom(\"in_obj\",[\"?obj1\",\"?reg1\"])), # object is not in region where place is attempted\n",
    "                               Atom(\"holding\",[\"?rob1\",\"?obj1\"]), # robot holds the object\n",
    "                               ],\n",
    "                effects=[Not(Atom(\"holding\",[\"?rob1\",\"?obj1\"])),Atom(\"in_obj\",[\"?obj1\",\"?reg1\"]),Atom(\"turn\",[\"?rob2\"]),Not(Atom(\"turn\",[\"?rob1\"]))],\n",
    "                execute_fn=deterministic_execute_fn,\n",
    "                effects_fn=deterministic_effects_fn,\n",
    "            ),\n",
    "            \n",
    "            ActionSchema(\n",
    "                name=\"clean\",\n",
    "                inputs=[\"?rob1\",\"?reg1\",\"?rob2\"],\n",
    "                input_types=[\"robot\",\"region\",\"robot\"],\n",
    "                preconditions=[Atom(\"turn\",[\"?rob1\"]),Not(Atom(\"turn\",[\"?rob2\"])),\n",
    "                               Not(Exists(Atom(\"in_obj\",[\"?obj\",\"?reg1\"]),[\"?obj\"],[\"physical\"])), # region is free\n",
    "                               Not(Atom(\"clean\",[\"?reg1\"])), # region is unclean\n",
    "                               Not(Exists(Atom(\"holding\",[\"?rob1\",\"?obj\"]),[\"?obj\"],[\"physical\"])), # robot hand is free\n",
    "                               ],\n",
    "                effects=[Atom(\"clean\",[\"?reg1\"]),Atom(\"turn\",[\"?rob2\"]),Not(Atom(\"turn\",[\"?rob1\"]))],\n",
    "                effects_fn=deterministic_effects_fn,\n",
    "                execute_fn=deterministic_execute_fn,\n",
    "            ),\n",
    "            \n",
    "            ActionSchema(\n",
    "                name=\"nothing\",\n",
    "                inputs=[\"?rob1\",\"?rob2\"],\n",
    "                input_types=[\"robot\",\"robot\"],\n",
    "                preconditions=[Atom(\"turn\",[\"?rob1\"]),Not(Atom(\"turn\",[\"?rob2\"]))],\n",
    "                effects=[Atom(\"turn\",[\"?rob2\"]),Not(Atom(\"turn\",[\"?rob1\"])),Atom(\"turn\",[\"?rob2\"]),Not(Atom(\"turn\",[\"?rob1\"]))],\n",
    "                effects_fn=deterministic_effects_fn,\n",
    "                execute_fn=deterministic_execute_fn,\n",
    "            ),\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        reward = GOAL\n",
    "\n",
    "        spec = ProblemSpec(\n",
    "            predicates=predicates,\n",
    "            action_schemas=action_schemas,\n",
    "            reward=reward,\n",
    "        )\n",
    "\n",
    "        return spec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "# Planner\n",
    "cfg = load_config(config_file=\"../tampura/config/default.yml\")\n",
    "\n",
    "# Set some print options to print out abstract belief, action, observation, and reward\n",
    "cfg[\"print_options\"] = \"ab,a,o,r\"\n",
    "cfg[\"vis_graph\"] = True\n",
    "# batch size 100, num samples 500 num skeletons 100 works best!!\n",
    "cfg[\"batch_size\"] = 100 #100 \n",
    "cfg[\"num_samples\"] = 100#500\n",
    "cfg[\"max_steps\"] = 15\n",
    "cfg[\"num_skeletons\"] = 10\n",
    "cfg[\"flat_sample\"] = False # TODO: check; may cause progressive widening\n",
    "cfg['save_dir'] = os.getcwd()+\"/runs/run{}\".format(time.time())\n",
    "\n",
    "# cfg['from_scratch'] = False # imp: re-use!!! but graph gets too big\n",
    "\n",
    "# TODO: check - can we reuse the same environment for both agents?\n",
    "# for robot1\n",
    "# Initialize environment\n",
    "env = ToyDiscreteCentral(config=cfg)\n",
    "b0, store= env.initialize(holding={ROBOTS[0]:[],ROBOTS[1]:[]},clean=[REGIONS[-1]],\n",
    "                          obj_regions={MUG:REGIONS[0]},turn=ROBOTS[0])\n",
    "\n",
    "# Set up logger to print info\n",
    "setup_logger(cfg[\"save_dir\"], logging.INFO)\n",
    "\n",
    "# Initialize the policy\n",
    "planner = TampuraPolicy(config = cfg, problem_spec = env.problem_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========t=0==========\n",
      "Abstract Belief: AbstractBelief(items=[Atom(pred_name='turn', args=['robot_1']), Atom(pred_name='in_obj', args=['mug', 'region_mug']), Atom(pred_name='clean', args=['region_stable_mug'])])\n",
      "Reward: 0.0\n",
      "[TampuraPolicy] Flat Action Parameter Sampling\n",
      "[TampuraPolicy] Outcome Sampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 210.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TampuraPolicy] MDP Solving\n",
      "Action: pick(robot_1, mug, region_mug, robot_2)\n",
      "Observation: Observation()\n",
      "\n",
      "==========t=1==========\n",
      "Abstract Belief: AbstractBelief(items=[Atom(pred_name='holding', args=['robot_1', 'mug']), Atom(pred_name='turn', args=['robot_2']), Atom(pred_name='clean', args=['region_stable_mug'])])\n",
      "Reward: 0.0\n",
      "[TampuraPolicy] MDP Solving\n",
      "Action: clean(robot_2, region_mug, robot_1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# should call execute function\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     bp \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mupdate(action, observation, store)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m planner\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvis\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/tamp_multi_agent/tampura/environment.py:65\u001b[0m, in \u001b[0;36mTampuraEnv.step\u001b[0;34m(self, action, belief, store)\u001b[0m\n\u001b[1;32m     62\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem_spec\u001b[38;5;241m.\u001b[39mget_action_schema(action\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Update the symbolic state\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, observation \u001b[38;5;241m=\u001b[39m \u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbelief\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m observation\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "b=b0\n",
    "assert env.problem_spec.verify(store)\n",
    "\n",
    "save_config(planner.config, planner.config[\"save_dir\"])\n",
    "\n",
    "history = RolloutHistory(planner.config)\n",
    "st = time.time()\n",
    "for step in range(100):\n",
    "# while True:\n",
    "    s = copy.deepcopy(env.state)\n",
    "    a_b = b.abstract(store)\n",
    "    reward = env.problem_spec.get_reward(a_b, store)\n",
    "    if reward:\n",
    "        print(\"goal achieved\")\n",
    "        break\n",
    "\n",
    "    logging.info(\"\\n\" + (\"=\" * 10) + \"t=\" + str(step) + (\"=\" * 10))\n",
    "    if \"s\" in planner.print_options:\n",
    "        logging.info(\"State: \" + str(s))\n",
    "    if \"b\" in planner.print_options:\n",
    "        logging.info(\"Belief: \" + str(b))\n",
    "    if \"ab\" in planner.print_options:\n",
    "        logging.info(\"Abstract Belief: \" + str(a_b))\n",
    "    if \"r\" in planner.print_options:\n",
    "        logging.info(\"Reward: \" + str(reward))\n",
    "    \n",
    "    \n",
    "    action, info, store = planner.get_action(b, store) # should only call effects functions!!??\n",
    "    \n",
    "    \n",
    "    if \"a\" in planner.print_options:\n",
    "        logging.info(\"Action: \" + str(action))\n",
    "\n",
    "    if action.name == \"no-op\":\n",
    "        bp = copy.deepcopy(b)\n",
    "        observation = None\n",
    "    else:\n",
    "        observation = env.step(action, b, store) # should call execute function\n",
    "        bp = b.update(action, observation, store)\n",
    "\n",
    "        if planner.config[\"vis\"]:\n",
    "            env.vis_updated_belief(bp, store)\n",
    "\n",
    "    a_bp = bp.abstract(store)\n",
    "    history.add(s, b, a_b, action, observation, reward, info, store, time.time() - st)\n",
    "\n",
    "    reward = env.problem_spec.get_reward(a_bp, store)\n",
    "    \n",
    "    if \"o\" in planner.print_options:\n",
    "        logging.info(\"Observation: \" + str(observation))\n",
    "    if \"sp\" in planner.print_options:\n",
    "        logging.info(\"Next State: \" + str(env.state))\n",
    "    if \"bp\" in planner.print_options:\n",
    "        logging.info(\"Next Belief: \" + str(bp))\n",
    "    if \"abp\" in planner.print_options:\n",
    "        logging.info(\"Next Abstract Belief: \" + str(a_bp))\n",
    "    if \"rp\" in planner.print_options:\n",
    "        logging.info(\"Next Reward: \" + str(reward))\n",
    "\n",
    "    # update the belief\n",
    "    b = bp\n",
    "\n",
    "history.add(env.state, bp, a_bp, None, None, reward, info, store, time.time() - st)\n",
    "\n",
    "logging.info(\"=\" * 20)\n",
    "\n",
    "env.wrapup()\n",
    "\n",
    "if not planner.config[\"real_execute\"]:\n",
    "    save_run_data(history, planner.config[\"save_dir\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_isaaclab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
